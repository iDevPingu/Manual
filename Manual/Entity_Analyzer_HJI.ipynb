{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "keras = tf.keras\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fit tokenizer to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "tokenizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 불러온다\n",
    "df = pd.read_csv('train_entity.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘 불러와졌나 확인\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일의 1행1열의 값을 넣으면 된다\n",
    "# 이 코드에서 불러온 train_entity는 1행1열이 word임\n",
    "tokenizer.fit(df['word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_char2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자마다 인덱스 줌\n",
    "wordvoca = tokenizer.char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvoca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity의 레이블과 id를 변환해주는 딕셔너리\n",
    "label_to_id = {t:i for i,t in enumerate(df.entity.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.entity.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자단위로 자른다. 최대 길이 10 짜리 리스트를 만든다\n",
    "# 글자가 '사과' 라면 2글자 이므로 나머지 8칸에는 _PAD_ 를 넣는다\n",
    "MAX_LENGTH = 10\n",
    "def tokenize_by_char(words,labels):\n",
    "    inputs , outputs = [], []\n",
    "    for word,label in zip(words,labels):\n",
    "        tempword = []\n",
    "        tempnum = []\n",
    "        for i in range(len(word)):\n",
    "            tempword.append(word[i])\n",
    "        for i in tempword:\n",
    "            tempnum.append(wordvoca[i])\n",
    "        if len(tempnum) <= MAX_LENGTH:\n",
    "            inputs.append(tempnum)\n",
    "            outputs.append(label_to_id[label])\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "        value = tokenizer.char2idx['_PAD_']) # value = 0\n",
    "  \n",
    "    return padded_inputs, outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자를 아까 만든 wordvoca에서의 인덱스 번호로 바꾼다\n",
    "def decode_num_char(wordvoca,inputs):\n",
    "    result = []\n",
    "    for i in range(len(inputs)):\n",
    "        for j in wordvoca.keys():\n",
    "            try:\n",
    "                if wordvoca[j] == inputs[i]:\n",
    "                    result.append(j)\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                result.append('')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tokenize_by_char(df.word, df.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', decode_num_char(wordvoca,inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 섞고 배치사이즈만큼 나눈다\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x, y)\n",
    "    print('-----------------------------------------------')\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 디자인한다 \n",
    "# 간단하게 임베딩 후 LSTM 으로 처리하고 활성화함수로는 relu를 사용했다\n",
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(wordvoca), 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_to_id.values()), activation='softmax')\n",
    "])\n",
    "    LEARNING_RATE = 0.001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 할 단어들을 wordvoca에 있는 인덱스로 key-value 설정한다\n",
    "def word_question_procession(words):\n",
    "    MAX_LENGTH = 10\n",
    "    inputs = [] \n",
    "    for word in words:\n",
    "        tempword = []\n",
    "        tempnum = []\n",
    "        for i in range(len(word)):\n",
    "            tempword.append(word[i])\n",
    "        for i in tempword:\n",
    "            try:\n",
    "                tempnum.append(wordvoca[i])\n",
    "            except:\n",
    "                pass\n",
    "        if len(tempnum) <= MAX_LENGTH:\n",
    "            inputs.append(tempnum)\n",
    "        else:\n",
    "            print(\"단어의 길이가 너무 길어요\")\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "    value = tokenizer.char2idx['_PAD_']) # value = 0\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word = word_question_procession(['빨간색','휴지','파랑색','비','무지개색','오른쪽','노란색','유니폼','상자','옆','위','뿡뿡색'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 해본다\n",
    "model.predict(input_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값을 id값으로 출력\n",
    "prediction = np.argmax(model.predict(input_word), axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id 값을 레이블 값으로 \n",
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장하기\n",
    "# model.save('entity_model.h5')\n",
    "# 모델 불러오기\n",
    "tempmodel = keras.models.load_model('entity_model.h5')\n",
    "tempmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.predict(input_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(np.argmax(tempmodel.predict(input_word), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
