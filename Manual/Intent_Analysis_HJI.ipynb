{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "keras = tf.keras\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fit tokenizer to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "tokenizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_intent.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인터넷 켜줘</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question    intent\n",
       "0   인터넷 켜줘  internet"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "64 terms are recognized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vectorizer.BaseVectorizer at 0x1efe01b1b48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit(df['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_PAD_': 0,\n",
       " '_UNK_': 1,\n",
       " '_STA_': 2,\n",
       " '_EOS_': 3,\n",
       " '인터넷': 4,\n",
       " '그림': 5,\n",
       " '봐': 6,\n",
       " '켜': 7,\n",
       " '좀': 8,\n",
       " '그리고': 9,\n",
       " '나': 10,\n",
       " '뭐': 11,\n",
       " '그릴': 12,\n",
       " '그림판': 13,\n",
       " '할래': 14,\n",
       " '크롬': 15,\n",
       " '틀어': 16,\n",
       " '그': 17,\n",
       " '리자': 18,\n",
       " '싶어': 19,\n",
       " '켜줘': 20,\n",
       " '이나': 21,\n",
       " '한': 22,\n",
       " '래': 23,\n",
       " '컴퓨터': 24,\n",
       " '로': 25,\n",
       " '알아보게': 26,\n",
       " '구글': 27,\n",
       " '음': 28,\n",
       " '이': 29,\n",
       " '싶다': 30,\n",
       " '만': 31,\n",
       " '거': 32,\n",
       " '오늘': 33,\n",
       " '은': 34,\n",
       " '그려': 35,\n",
       " '보자': 36,\n",
       " '지금': 37,\n",
       " '심심한데': 38,\n",
       " '홈페이지': 39,\n",
       " '띄워': 40,\n",
       " '하자': 41,\n",
       " '번': 42,\n",
       " '할거야': 43,\n",
       " '그릴거야': 44,\n",
       " '틀어줘': 45,\n",
       " '그려야': 46,\n",
       " '징': 47,\n",
       " '그려야지': 48,\n",
       " '하고싶은데': 49,\n",
       " '하고': 50,\n",
       " '할': 51,\n",
       " '수': 52,\n",
       " '있나': 53,\n",
       " '하고싶네': 54,\n",
       " '할까': 55,\n",
       " '웹서핑': 56,\n",
       " '검색': 57,\n",
       " '하게': 58,\n",
       " '그리게': 59,\n",
       " '싶은데': 60,\n",
       " '싶네': 61,\n",
       " '아': 62,\n",
       " '그려야겠다': 63}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {t:i for i,t in enumerate(df.intent.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.intent.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'internet': 0, 'paint': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.intent = df.intent.map(lambda x : label_index[x])\n",
    "# print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "def tokenize_and_filter(sentences, labels):\n",
    "    inputs, outputs = [], []\n",
    "  \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "            outputs.append(label_to_id[label])\n",
    "  \n",
    "      # pad tokenized sentences\n",
    "        padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "            value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "  \n",
    "    return padded_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tokenize_and_filter(df.question, df.intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input :  [ 4 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] label :  0 original input sentence :  ['인터넷', '켜줘', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_']\n"
     ]
    }
   ],
   "source": [
    "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', tokenizer.decode_from_list(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 5 48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [33 34  5 21 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5 46 47  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [28  4 21 55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  7  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [62 63  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  9 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5 12 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10  5  9 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10 24 25  5  9 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5 12 31 22 32 16  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11  8 26 15  7  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [37  5 29  9 61  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11  8 26  4  8  7  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 9 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(16, 40), dtype=int32) tf.Tensor([1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1], shape=(16,), dtype=int32)\n",
      "-----------------------------------------------\n",
      "(16, 40) (16,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x, y)\n",
    "    print('-----------------------------------------------')\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(label_to_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.n_vocabs, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_to_id.values()), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6761 - sparse_categorical_accuracy: 0.6792\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6753 - sparse_categorical_accuracy: 0.6604\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6744 - sparse_categorical_accuracy: 0.6226\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6725 - sparse_categorical_accuracy: 0.6415\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6716 - sparse_categorical_accuracy: 0.7358\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6695 - sparse_categorical_accuracy: 0.8302\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6680 - sparse_categorical_accuracy: 0.8302\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6627 - sparse_categorical_accuracy: 0.8302\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6633 - sparse_categorical_accuracy: 0.9057\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6619 - sparse_categorical_accuracy: 0.9245\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6652 - sparse_categorical_accuracy: 0.875 - 0s 15ms/step - loss: 0.6597 - sparse_categorical_accuracy: 0.9623\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6564 - sparse_categorical_accuracy: 0.9623\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6542 - sparse_categorical_accuracy: 0.9434\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6484 - sparse_categorical_accuracy: 0.9057\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6447 - sparse_categorical_accuracy: 0.9245\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6448 - sparse_categorical_accuracy: 0.9623\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6389 - sparse_categorical_accuracy: 0.9811\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6382 - sparse_categorical_accuracy: 0.9811\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.9811\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6293 - sparse_categorical_accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eff0e35508>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_processing(sentences):\n",
    "    inputs = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "        else:\n",
    "            print('입력이 너무 길어요.')\n",
    "    # pad tokenized sentences\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "    value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = question_processing(['인터넷 한 번 켜봐', \n",
    "                                      '그림 그리게 인터넷 켜줘',\n",
    "                                      '인터넷이나 한 번 해볼까',\n",
    "                                      '음.. 그림 그려볼까',\n",
    "                                      '인터넷 ㄱㄱ'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53525996, 0.46474007],\n",
       "       [0.48697442, 0.51302564],\n",
       "       [0.52566403, 0.4743359 ],\n",
       "       [0.47329047, 0.52670956],\n",
       "       [0.51432174, 0.48567832]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "prediction = np.argmax(model.predict(input_sentence), axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internet\n",
      "paint\n",
      "internet\n",
      "paint\n",
      "internet\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장하기\n",
    "# model.save('entitytestmodel.h5')\n",
    "# 모델 불러오기\n",
    "tempmodel = keras.models.load_model('intent_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
