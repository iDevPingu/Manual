{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "keras = tf.keras\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fit tokenizer to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "vectorizer = BaseVectorizer(t.morphs)\n",
    "tokenizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_entity.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>빨간색</td>\n",
       "      <td>color</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word entity\n",
       "0  빨간색  color"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "105 terms are recognized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vectorizer.BaseVectorizer at 0x1f8b1055cc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit(df['word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_char2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvoca = tokenizer.char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_PAD_': 0,\n",
       " '_UNK_': 1,\n",
       " '_': 2,\n",
       " 'P': 3,\n",
       " 'A': 4,\n",
       " 'D': 5,\n",
       " 'U': 6,\n",
       " 'N': 7,\n",
       " 'K': 8,\n",
       " 'S': 9,\n",
       " 'T': 10,\n",
       " 'E': 11,\n",
       " 'O': 12,\n",
       " '색': 13,\n",
       " '보': 14,\n",
       " '라': 15,\n",
       " '공': 16,\n",
       " '과': 17,\n",
       " '자': 18,\n",
       " '지': 19,\n",
       " '갑': 20,\n",
       " '위': 21,\n",
       " '아': 22,\n",
       " '래': 23,\n",
       " '맨': 24,\n",
       " '빨': 25,\n",
       " '간': 26,\n",
       " '파': 27,\n",
       " '랑': 28,\n",
       " '강': 29,\n",
       " '란': 30,\n",
       " '주': 31,\n",
       " '황': 32,\n",
       " '노': 33,\n",
       " '초': 34,\n",
       " '록': 35,\n",
       " '녹': 36,\n",
       " '연': 37,\n",
       " '두': 38,\n",
       " '남': 39,\n",
       " '하': 40,\n",
       " '늘': 41,\n",
       " '분': 42,\n",
       " '홍': 43,\n",
       " '청': 44,\n",
       " '적': 45,\n",
       " '갈': 46,\n",
       " '네': 47,\n",
       " '이': 48,\n",
       " '비': 49,\n",
       " '동': 50,\n",
       " '차': 51,\n",
       " '축': 52,\n",
       " '구': 53,\n",
       " '농': 54,\n",
       " '피': 55,\n",
       " '럭': 56,\n",
       " '야': 57,\n",
       " '테': 58,\n",
       " '니': 59,\n",
       " '스': 60,\n",
       " '장': 61,\n",
       " '난': 62,\n",
       " '감': 63,\n",
       " '의': 64,\n",
       " '책': 65,\n",
       " '상': 66,\n",
       " '컵': 67,\n",
       " '박': 68,\n",
       " '게': 69,\n",
       " '임': 70,\n",
       " '기': 71,\n",
       " '컴': 72,\n",
       " '퓨': 73,\n",
       " '터': 74,\n",
       " '텔': 75,\n",
       " '레': 76,\n",
       " '전': 77,\n",
       " '티': 78,\n",
       " '가': 79,\n",
       " '방': 80,\n",
       " '양': 81,\n",
       " '말': 82,\n",
       " '패': 83,\n",
       " '딩': 84,\n",
       " '외': 85,\n",
       " '투': 86,\n",
       " '셔': 87,\n",
       " '츠': 88,\n",
       " '바': 89,\n",
       " '핸': 90,\n",
       " '드': 91,\n",
       " '폰': 92,\n",
       " '휴': 93,\n",
       " '대': 94,\n",
       " '키': 95,\n",
       " '마': 96,\n",
       " '우': 97,\n",
       " '모': 98,\n",
       " '트': 99,\n",
       " '북': 100,\n",
       " '오': 101,\n",
       " '른': 102,\n",
       " '쪽': 103,\n",
       " '왼': 104,\n",
       " '좌': 105,\n",
       " '회': 106,\n",
       " '측': 107,\n",
       " '저': 108,\n",
       " '여': 109,\n",
       " '서': 110,\n",
       " '옆': 111,\n",
       " '안': 112,\n",
       " '붉': 113,\n",
       " '은': 114,\n",
       " '푸': 115,\n",
       " '누': 116,\n",
       " '런': 117,\n",
       " '르': 118,\n",
       " '름': 119,\n",
       " '한': 120,\n",
       " '유': 121,\n",
       " '손': 122,\n",
       " '목': 123,\n",
       " '시': 124,\n",
       " '계': 125,\n",
       " '물': 126,\n",
       " '슈': 127,\n",
       " '커': 128,\n",
       " '포': 129}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvoca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {t:i for i,t in enumerate(df.entity.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.entity.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'color': 0, 'thing': 1, 'loc': 2}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "def tokenize_by_char(words,labels):\n",
    "    inputs , outputs = [], []\n",
    "    for word,label in zip(words,labels):\n",
    "        tempword = []\n",
    "        tempnum = []\n",
    "        for i in range(len(word)):\n",
    "            tempword.append(word[i])\n",
    "        for i in tempword:\n",
    "            tempnum.append(wordvoca[i])\n",
    "        if len(tempnum) <= MAX_LENGTH:\n",
    "            inputs.append(tempnum)\n",
    "            outputs.append(label_to_id[label])\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "        value = tokenizer.char2idx['_PAD_']) # value = 0\n",
    "  \n",
    "    return padded_inputs, outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_num_char(wordvoca,inputs):\n",
    "    result = []\n",
    "    for i in range(len(inputs)):\n",
    "        for j in wordvoca.keys():\n",
    "            try:\n",
    "                if wordvoca[j] == inputs[i]:\n",
    "                    result.append(j)\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                result.append('')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tokenize_by_char(df.word, df.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input :  [25 26 13  0  0  0  0  0  0  0] label :  0 original input sentence :  ['빨', '간', '색', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_']\n"
     ]
    }
   ],
   "source": [
    "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', decode_num_char(wordvoca,inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 10), (None,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 65   0   0   0   0   0   0   0   0   0]\n",
      " [ 93  94  92   0   0   0   0   0   0   0]\n",
      " [ 69  70  71   0   0   0   0   0   0   0]\n",
      " [ 31  32  13   0   0   0   0   0   0   0]\n",
      " [128  55 129  99   0   0   0   0   0   0]\n",
      " [115 118  60 119 120   0   0   0   0   0]\n",
      " [110 103   0   0   0   0   0   0   0   0]\n",
      " [100 103   0   0   0   0   0   0   0   0]\n",
      " [ 34  35  13   0   0   0   0   0   0   0]\n",
      " [ 58  59  60  16   0   0   0   0   0   0]\n",
      " [ 98  59  74   0   0   0   0   0   0   0]\n",
      " [ 34  35   0   0   0   0   0   0   0   0]\n",
      " [ 81  82   0   0   0   0   0   0   0   0]\n",
      " [ 27  30   0   0   0   0   0   0   0   0]\n",
      " [ 54  53  16   0   0   0   0   0   0   0]\n",
      " [ 27  30  13   0   0   0   0   0   0   0]], shape=(16, 10), dtype=int32) tf.Tensor([1 1 1 0 1 0 2 2 0 1 1 0 1 0 1 0], shape=(16,), dtype=int32)\n",
      "-----------------------------------------------\n",
      "(16, 10) (16,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x, y)\n",
    "    print('-----------------------------------------------')\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(label_to_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvoca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(wordvoca), 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_to_id.values()), activation='softmax')\n",
    "])\n",
    "    LEARNING_RATE = 0.001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.0930 - sparse_categorical_accuracy: 0.4141\n",
      "Epoch 2/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0660 - sparse_categorical_accuracy: 0.4646\n",
      "Epoch 3/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0448 - sparse_categorical_accuracy: 0.4646\n",
      "Epoch 4/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0100 - sparse_categorical_accuracy: 0.4646\n",
      "Epoch 5/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9739 - sparse_categorical_accuracy: 0.4646\n",
      "Epoch 6/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8999 - sparse_categorical_accuracy: 0.6061\n",
      "Epoch 7/15\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8010 - sparse_categorical_accuracy: 0.7172\n",
      "Epoch 8/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6226 - sparse_categorical_accuracy: 0.7374\n",
      "Epoch 9/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4342 - sparse_categorical_accuracy: 0.7374\n",
      "Epoch 10/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2832 - sparse_categorical_accuracy: 0.9697\n",
      "Epoch 11/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1189 - sparse_categorical_accuracy: 0.9697\n",
      "Epoch 12/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0700 - sparse_categorical_accuracy: 0.9798\n",
      "Epoch 13/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0102 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0039 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f8d3d13a48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_question_procession(words):\n",
    "    MAX_LENGTH = 10\n",
    "    inputs = [] \n",
    "    for word in words:\n",
    "        tempword = []\n",
    "        tempnum = []\n",
    "        for i in range(len(word)):\n",
    "            tempword.append(word[i])\n",
    "        for i in tempword:\n",
    "            try:\n",
    "                tempnum.append(wordvoca[i])\n",
    "            except:\n",
    "                pass\n",
    "        if len(tempnum) <= MAX_LENGTH:\n",
    "            inputs.append(tempnum)\n",
    "        else:\n",
    "            print(\"단어의 길이가 너무 길어요\")\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "    value = tokenizer.char2idx['_PAD_']) # value = 0\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = word_question_procession(['빨간색','휴지','파랑색','비','무지개색','오른쪽','노란색','유니폼','상자','옆','위','뿡뿡색'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9997067e-01, 2.9332679e-05, 2.1947630e-09],\n",
       "       [5.4008409e-04, 9.9943358e-01, 2.6312398e-05],\n",
       "       [9.9996519e-01, 3.4757668e-05, 2.3448181e-09],\n",
       "       [1.5138368e-04, 9.9966860e-01, 1.8008229e-04],\n",
       "       [9.8718774e-01, 1.2812126e-02, 1.4352683e-07],\n",
       "       [1.5563320e-06, 6.1741233e-04, 9.9938095e-01],\n",
       "       [9.9995470e-01, 4.5310891e-05, 1.3613193e-09],\n",
       "       [2.4302609e-03, 9.9754083e-01, 2.8908778e-05],\n",
       "       [1.7364405e-03, 9.9825102e-01, 1.2494587e-05],\n",
       "       [2.8220842e-07, 6.4146298e-04, 9.9935824e-01],\n",
       "       [2.9581034e-07, 5.2315136e-04, 9.9947661e-01],\n",
       "       [9.9108011e-01, 8.9190099e-03, 9.7126122e-07]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 2 0 1 1 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "prediction = np.argmax(model.predict(input_sentence), axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n",
      "thing\n",
      "color\n",
      "thing\n",
      "color\n",
      "loc\n",
      "color\n",
      "thing\n",
      "thing\n",
      "loc\n",
      "loc\n",
      "color\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          8320      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 82,819\n",
      "Trainable params: 82,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 모델 저장하기\n",
    "# model.save('entitytestmodel.h5')\n",
    "# 모델 불러오기\n",
    "tempmodel = keras.models.load_model('entity_model.h5')\n",
    "tempmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99997258e-01, 2.36760138e-06, 3.79820023e-07],\n",
       "       [4.01552606e-05, 9.99946475e-01, 1.33432895e-05],\n",
       "       [9.99997973e-01, 1.85279464e-06, 1.44728048e-07],\n",
       "       [1.45368362e-02, 9.83871937e-01, 1.59128278e-03],\n",
       "       [7.20760524e-01, 2.78951228e-01, 2.88266368e-04],\n",
       "       [6.82854501e-04, 3.85420630e-04, 9.98931706e-01],\n",
       "       [9.99993563e-01, 6.26468636e-06, 1.59969190e-07],\n",
       "       [2.25275682e-04, 9.99717891e-01, 5.68391297e-05],\n",
       "       [7.61514675e-05, 9.99898911e-01, 2.49588320e-05],\n",
       "       [4.26918385e-04, 3.24093970e-04, 9.99248922e-01],\n",
       "       [4.10627166e-04, 3.02611763e-04, 9.99286830e-01],\n",
       "       [9.87252474e-01, 9.90190543e-03, 2.84563052e-03]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempmodel.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(np.argmax(tempmodel.predict(input_sentence), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 2, 0, 1, 1, 2, 2, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
