{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "keras = tf.keras\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fit tokenizer to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "tokenizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 불러온다\n",
    "df = pd.read_csv('train_intent.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘 불러와졌나 확인\n",
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일의 1행1열의 값을 넣으면 된다\n",
    "# 이 코드에서 불러온 train_intent는 1행1열이 question임\n",
    "tokenizer.fit(df['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 단어별로 나누고 단어에 id 부여\n",
    "tokenizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent의 레이블과 id를 변환해주는 딕셔너리\n",
    "label_to_id = {t:i for i,t in enumerate(df.intent.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.intent.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위로 나눈다\n",
    "# 만약 \"인터넷 좀 켜줘\" 였다면 인터넷, 좀, 켜, 줘 정도로 나누고 \n",
    "# 최대길이 20 이하의 부분은 _PAD_ 로처리한다\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "def tokenize_and_filter(sentences, labels):\n",
    "    inputs, outputs = [], []\n",
    "  \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "            outputs.append(label_to_id[label])\n",
    "  \n",
    "      # pad tokenized sentences\n",
    "        padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "            value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "  \n",
    "    return padded_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tokenize_and_filter(df.question, df.intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', tokenizer.decode_from_list(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 섞고 배치사이즈만큼 나눈다\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x, y)\n",
    "    print('-----------------------------------------------')\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_to_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 디자인한다 \n",
    "# 간단하게 임베딩 후 LSTM 으로 처리하고 활성화함수로는 relu를 사용했다\n",
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.n_vocabs, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_to_id.values()), activation='softmax')\n",
    "])\n",
    "    LEARNING_RATE = 0.0001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킴\n",
    "model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 할 문장들을 단어 단위로 쪼개서 vocabulary에 있는 인덱스로 key-value 설정한다\n",
    "\n",
    "def question_processing(sentences):\n",
    "    inputs = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "        else:\n",
    "            print('입력이 너무 길어요.')\n",
    "\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "    value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = question_processing(['인터넷 한 번 켜봐', \n",
    "                                      '그림 그리게 인터넷 켜줘',\n",
    "                                      '인터넷이나 한 번 해볼까',\n",
    "                                      '음.. 그림 그려볼까',\n",
    "                                      '인터넷 ㄱㄱ'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 해본다\n",
    "model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값을 id값으로 출력\n",
    "prediction = np.argmax(model.predict(input_sentence), axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id 값을 레이블 값으로 \n",
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장하기\n",
    "# model.save('intent_model.h5')\n",
    "# 모델 불러오기\n",
    "tempmodel = keras.models.load_model('intent_model.h5')\n",
    "tempmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempmodel.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(np.argmax(tempmodel.predict(input_sentence), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
